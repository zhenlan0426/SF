{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from functions import MLP_generator,test_MLP_generator,createGraphANN\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "seq_len = 16 # same as test set for convenience\n",
    "learning_rate = 1e-4\n",
    "epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time dependent list of discrete variables of shape (B,T)\n",
    "discreteList = ['dayOfWeek','payDay','month','earthquake','type','locale','locale_name','transferred','onpromotion']\n",
    "# time independent list of discrete variables of shape of shape (B,)\n",
    "disc_X = ['store_nbr','item_nbr',\n",
    " 'family', 'class',\n",
    " 'perishable', 'city',\n",
    " 'state', 'type', 'cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinalitys_X = [55, 4001, 34, 337, 2, 23, 17, 6, 18]\n",
    "cardinalitys_T = [7, 2, 13, 2, 7, 4, 25, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimentions_X = [2, 20, 1, 2, 1, 1, 1, 1, 1]\n",
    "dimentions_T = [1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dX = sum(dimentions_X)\n",
    "dT = sum(dimentions_T)\n",
    "d = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.array([2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) * 5e-4 # regularization coefficient\n",
    "l1,l2,l3 = 5e-4,5e-4,5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Build Computational Graph **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs,train_op,cost,saver,yhat = createGraphANN(batch_size,seq_len,cardinalitys_X,cardinalitys_T,\\\n",
    "                                        dimentions_X,dimentions_T,dX,dT,d,lambdas,l1,l2,l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monitor = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_np = np.loadtxt(prefix+'_Y',dtype=np.float32, delimiter=\",\") \n",
    "weight_np = np.loadtxt(prefix+'_Weight',dtype=np.float32, delimiter=\",\") \n",
    "Con_np = np.loadtxt(prefix+'_Con',dtype=np.float32, delimiter=\",\") \n",
    "X_np = np.loadtxt(prefix+'_X',dtype=np.int32,delimiter=\",\") \n",
    "Count_np = np.loadtxt(prefix+'_Count',dtype=np.int32,delimiter=\",\") \n",
    "Dis_np = [np.loadtxt(prefix+'_Dis'+str(j),dtype=np.int32,delimiter=\",\")  for j in range(len(discreteList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_np2 = np.loadtxt(prefix+'_Y',dtype=np.float32, delimiter=\",\") \n",
    "weight_np2 = np.loadtxt(prefix+'_Weight',dtype=np.float32, delimiter=\",\") \n",
    "Con_np2 = np.loadtxt(prefix+'_Con',dtype=np.float32, delimiter=\",\") \n",
    "X_np2 = np.loadtxt(prefix+'_X',dtype=np.int32,delimiter=\",\") \n",
    "Count_np2 = np.loadtxt(prefix+'_Count',dtype=np.int32,delimiter=\",\") \n",
    "Dis_np2 = [np.loadtxt(prefix+'_Dis'+str(j),dtype=np.int32,delimiter=\",\")  for j in range(len(discreteList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:1.72163075426, Val loss:1.80699790054\n",
      "Train loss:0.826315430584, Val loss:0.914323050991\n",
      "Train loss:0.905139372221, Val loss:0.920353949345\n",
      "Train loss:0.940586009691, Val loss:0.9066888186\n",
      "Train loss:0.813488642837, Val loss:0.897219107901\n",
      "Train loss:0.76411109071, Val loss:0.886174819001\n",
      "Train loss:1.03361928655, Val loss:0.882708641081\n",
      "Train loss:0.725109692347, Val loss:0.876742720611\n",
      "Train loss:0.889341997174, Val loss:0.870037204655\n",
      "Train loss:0.711028750132, Val loss:0.861466088819\n",
      "Train loss:0.691384334558, Val loss:0.848129772192\n",
      "Train loss:0.664474569449, Val loss:0.829221116824\n",
      "Train loss:0.597104921712, Val loss:0.809831425283\n",
      "Train loss:0.793219392997, Val loss:0.797145638729\n",
      "Train loss:0.713668960322, Val loss:0.789307732898\n",
      "Train loss:0.554202483128, Val loss:0.784280726634\n",
      "Train loss:0.536369294145, Val loss:0.782963767628\n",
      "Train loss:0.570822527581, Val loss:0.783045869187\n",
      "Train loss:0.723891124296, Val loss:0.779279875493\n",
      "Train loss:0.650999625192, Val loss:0.7787386454\n",
      "Train loss:0.669602610143, Val loss:0.775123516526\n",
      "Train loss:0.635362251259, Val loss:0.778479020698\n",
      "Train loss:0.56674220225, Val loss:0.776452531024\n",
      "Train loss:0.708581878391, Val loss:0.778336096214\n",
      "Train loss:0.640395132038, Val loss:0.777884354087\n",
      "Train loss:0.611764629916, Val loss:0.781507107172\n",
      "Train loss:0.626313371303, Val loss:0.787768277378\n",
      "Train loss:0.658083359035, Val loss:0.78230135264\n",
      "Train loss:0.612359784153, Val loss:0.780232877025\n",
      "Train loss:0.646507322482, Val loss:0.782030941291\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    cost_train = .0\n",
    "    weight_train = .0\n",
    "    cost_val = .0\n",
    "    weight_val = .0\n",
    "    for j,X_nps in enumerate(MLP_generator(y_np, weight_np,Con_np,Dis_np,X_np,Count_np,\\\n",
    "                              batch_size,seq_len,shuffle=True,downSample=0.5)):\n",
    "        _,cost_np = sess.run([train_op,cost],dict(zip(inputs,X_nps+[learning_rate])))\n",
    "        cost_train += cost_np\n",
    "        weight_train = weight_train + np.sum(X_nps[1])\n",
    "        \n",
    "        if j%monitor == 0:\n",
    "            for X_nps in MLP_generator(y_np2, weight_np2,Con_np2,Dis_np2,X_np2,Count_np2,\\\n",
    "                                       batch_size,seq_len,shuffle=False,downSample=0.5):\n",
    "                cost_np = sess.run([cost],dict(zip(inputs,X_nps)))\n",
    "                cost_val += cost_np[0]       \n",
    "                weight_val = weight_val + np.sum(X_nps[1])\n",
    "        \n",
    "            print \"Train loss:{}, Val loss:{}\".format(np.sqrt(seq_len*batch_size*cost_train/weight_train),\\\n",
    "                                                      np.sqrt(seq_len*batch_size*cost_val/weight_val))\n",
    "            cost_train = .0\n",
    "            weight_train = .0\n",
    "            cost_val = .0\n",
    "            weight_val = .0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANN_fillin_01'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model parameters\n",
    "saver.save(sess,'ANN_fillin_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Prediction ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs,train_op,cost,saver,yhat = createGraphANN(None,seq_len,cardinalitys_X,cardinalitys_T,\\\n",
    "                                              dimentions_X,dimentions_T,dX,dT,d,lambdas,l1,l2,l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ANN_fillin_01\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'ANN_fillin_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'test_SI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_np_SI = np.loadtxt(prefix+'_Y',dtype=np.int32, delimiter=\",\") \n",
    "Con_np_SI = np.loadtxt(prefix+'_Con',dtype=np.float32, delimiter=\",\") \n",
    "X_np_SI = np.loadtxt(prefix+'_X',dtype=np.int32,delimiter=\",\") \n",
    "Dis_np_SI = [np.loadtxt(prefix+'_Dis'+str(j),dtype=np.int32,delimiter=\",\")  for j in range(len(discreteList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'test_newItem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_np_newItem = np.loadtxt(prefix+'_Y',dtype=np.int32, delimiter=\",\") \n",
    "Con_np_newItem = np.loadtxt(prefix+'_Con',dtype=np.float32, delimiter=\",\") \n",
    "X_np_newItem = np.loadtxt(prefix+'_X',dtype=np.int32,delimiter=\",\") \n",
    "Dis_np_newItem = [np.loadtxt(prefix+'_Dis'+str(j),dtype=np.int32,delimiter=\",\")  for j in range(len(discreteList))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_SI, ID_SI = [],[]\n",
    "for y,x in test_MLP_generator(y_np_SI, Con_np_SI,Dis_np_SI,X_np_SI,batch_size):\n",
    "    ID_SI.append(np.reshape(y,(-1)))\n",
    "    yhat_SI.append(np.reshape(sess.run(yhat,dict(zip(inputs[2:-1],x))),(-1)))\n",
    "SI = pd.DataFrame({'id':np.concatenate(ID_SI),'unit_sales':np.concatenate(yhat_SI)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_newItem, ID_newItem = [],[]\n",
    "for y,x in test_MLP_generator(y_np_newItem, Con_np_newItem,Dis_np_newItem,X_np_newItem,batch_size):\n",
    "    ID_newItem.append(np.reshape(y,(-1)))\n",
    "    yhat_newItem.append(np.reshape(sess.run(yhat,dict(zip(inputs[2:-1],x))),(-1)))\n",
    "newItem = pd.DataFrame({'id':np.concatenate(ID_newItem),'unit_sales':np.concatenate(yhat_newItem)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeroSubmission = pd.read_csv('zeroSubmission.csv',dtype={'id': 'int32','unit_sales': 'float32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.concat([SI,newItem,zeroSubmission]).sort_values(['id']).to_csv('ANN-submit1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission = pd.read_csv('sample_submission.csv',dtype={'id': 'int32','unit_sales': 'float32'})\n",
    "#forecastId = SI.id.tolist() + newItem.id.tolist()\n",
    "#submission[~submission.id.isin(forecastId)].to_csv('zeroSubmission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
